#### Neural networks are computational models that serve as a fundamental component of artificial intelligence (AI) and machine learning (ML). Neural networks make decisions in a similar pattern to the human brain and can be trained by datasets to derive conclusions. Over time, as models are created based on datasets and information, neural networks improve their accuracy. For example, Google’s search engine is a neural network. 

<br>

## **Types of neural networks include:**
* Convolutional Neural Networks (CNNs)
* Recurrent Neural Networks (RNNs)


<br>

## **History of Neural Networks**

* In 1943, Warren S. McCulloch and Walter Pitts published groundbreaking research on the logical operations of neurons in the brain, likening them to binary threshold units akin to Boolean logic.

* In 1958, Frank Rosenblatt advanced this work with the invention of the perceptron, a model for storing and organizing information in the brain. He introduced the concept of weights, enabling computers, such as the IBM 704, to learn tasks like distinguishing between marked cards.

* By 1974, Paul Werbos furthered the field with his exploration of backpropagation in neural networks, first noted in his PhD thesis, a pivotal concept for optimizing learning processes within these systems.

* In 1989, Yann LeCun integrated backpropagation with neural network architectures, utilizing constraints to successfully train algorithms for recognizing hand-written zip code digits, a breakthrough in practical machine learning applications.

<br>

## **Key components of Neural Networks:**
### Neurons 
At the core of neural networks are neurons that receive input signals. These neurons go through a series of steps to reach an output. 

<br>

### Layers 
#### There are 3 layers the neurons travel through, input: 
#### 1. Input layer
   * Raw data is fed into the neural network
#### 2. Hidden layer
   * Complex computations and transformations of data 
   * Uses weights and activation functions
   * It may include multiple hidden layers
#### 3. Output
   * Produce the neural network’s final output
   * May use activation functions such as Binary Classification or Multi-class Classification

<br>

### Weights/Biases
Often in hidden layers of neural networks, weights are assigned to input neurons. These inputs are multiplied by assigned weights, summed up, and applied to a threshold. If the sum exceeds this threshold, the neurons activate and pass data to nodes in the next layer. This sum is then passed through an activation function, determining the node's output. The activation function introduces non-linearity into the output, allowing the neural network to learn and model complex relationships in the data. Weights and biases are constantly adjusted as the model continues to be trained, allowing for fewer mistakes and more accurate outputs. 

<br>

### Activation function (more details in other articles)
Activation functions are mathematical functions applied to get the output of a neuron, similar to a filter. Types of activation functions include Sigmoid function, Softmax function, ReLU, Leaky ReLU, and Tanh. 

<br>

## **Why Are Neural Networks Important?**
Neural networks analyze large amounts of data, recognizing patterns and correlations, allowing computers to make intelligent decisions with minimal human assistance. Neural networks are the frameworks for AI and ML and are largely inspired by the human brain itself. 

<br>

### *References*
Hardesty, L. (2017, April 14). Explained: Neural networks | MIT News | Massachusetts Institute of Technology. MIT News. Retrieved July 16, 2024, from https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414

What is a Neural Network? (n.d.). IBM. Retrieved July 16, 2024, from https://www.ibm.com/topics/neural-networks
